{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlbertConfig: (None, <class 'transformers.models.albert.tokenization_albert_fast.AlbertTokenizerFast'>)\n",
      "AlignConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "AriaConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "AyaVisionConfig: (None, <class 'transformers.models.cohere.tokenization_cohere_fast.CohereTokenizerFast'>)\n",
      "BarkConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "BartConfig: (<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>, <class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>)\n",
      "BertConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "BertGenerationConfig: (None, None)\n",
      "BigBirdConfig: (None, <class 'transformers.models.big_bird.tokenization_big_bird_fast.BigBirdTokenizerFast'>)\n",
      "BigBirdPegasusConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.PegasusTokenizer'>, <class 'transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast'>)\n",
      "BioGptConfig: (<class 'transformers.models.biogpt.tokenization_biogpt.BioGptTokenizer'>, None)\n",
      "BlenderbotConfig: (<class 'transformers.models.blenderbot.tokenization_blenderbot.BlenderbotTokenizer'>, <class 'transformers.models.blenderbot.tokenization_blenderbot_fast.BlenderbotTokenizerFast'>)\n",
      "BlenderbotSmallConfig: (<class 'transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer'>, None)\n",
      "BlipConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "Blip2Config: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "BloomConfig: (None, <class 'transformers.models.bloom.tokenization_bloom_fast.BloomTokenizerFast'>)\n",
      "BridgeTowerConfig: (<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, <class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>)\n",
      "BrosConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "CamembertConfig: (None, <class 'transformers.models.camembert.tokenization_camembert_fast.CamembertTokenizerFast'>)\n",
      "CanineConfig: (<class 'transformers.models.canine.tokenization_canine.CanineTokenizer'>, None)\n",
      "ChameleonConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "ChineseCLIPConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "ClapConfig: (<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, <class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>)\n",
      "CLIPConfig: (<class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>, <class 'transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast'>)\n",
      "CLIPSegConfig: (<class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>, <class 'transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast'>)\n",
      "ClvpConfig: (<class 'transformers.models.clvp.tokenization_clvp.ClvpTokenizer'>, None)\n",
      "LlamaConfig: (None, <class 'transformers.models.code_llama.tokenization_code_llama_fast.CodeLlamaTokenizerFast'>)\n",
      "CodeGenConfig: (<class 'transformers.models.codegen.tokenization_codegen.CodeGenTokenizer'>, <class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>)\n",
      "CohereConfig: (None, <class 'transformers.models.cohere.tokenization_cohere_fast.CohereTokenizerFast'>)\n",
      "Cohere2Config: (None, <class 'transformers.models.cohere.tokenization_cohere_fast.CohereTokenizerFast'>)\n",
      "ColPaliConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "ConvBertConfig: (<class 'transformers.models.convbert.tokenization_convbert.ConvBertTokenizer'>, <class 'transformers.models.convbert.tokenization_convbert_fast.ConvBertTokenizerFast'>)\n",
      "CpmAntConfig: (<class 'transformers.models.cpmant.tokenization_cpmant.CpmAntTokenizer'>, None)\n",
      "CTRLConfig: (<class 'transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer'>, None)\n",
      "Data2VecAudioConfig: (<class 'transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer'>, None)\n",
      "Data2VecTextConfig: (<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, <class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>)\n",
      "DbrxConfig: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "DebertaConfig: (<class 'transformers.models.deberta.tokenization_deberta.DebertaTokenizer'>, <class 'transformers.models.deberta.tokenization_deberta_fast.DebertaTokenizerFast'>)\n",
      "DebertaV2Config: (None, <class 'transformers.models.deberta_v2.tokenization_deberta_v2_fast.DebertaV2TokenizerFast'>)\n",
      "DeepseekV3Config: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "DiffLlamaConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "DistilBertConfig: (<class 'transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer'>, <class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>)\n",
      "DPRConfig: (<class 'transformers.models.dpr.tokenization_dpr.DPRQuestionEncoderTokenizer'>, <class 'transformers.models.dpr.tokenization_dpr_fast.DPRQuestionEncoderTokenizerFast'>)\n",
      "ElectraConfig: (<class 'transformers.models.electra.tokenization_electra.ElectraTokenizer'>, <class 'transformers.models.electra.tokenization_electra_fast.ElectraTokenizerFast'>)\n",
      "Emu3Config: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "ErnieConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "ErnieMConfig: (None, None)\n",
      "EsmConfig: (<class 'transformers.models.esm.tokenization_esm.EsmTokenizer'>, None)\n",
      "FalconConfig: (None, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>)\n",
      "FalconMambaConfig: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "FastSpeech2ConformerConfig: (None, None)\n",
      "FlaubertConfig: (<class 'transformers.models.flaubert.tokenization_flaubert.FlaubertTokenizer'>, None)\n",
      "FNetConfig: (<class 'transformers.models.fnet.tokenization_fnet.FNetTokenizer'>, <class 'transformers.models.fnet.tokenization_fnet_fast.FNetTokenizerFast'>)\n",
      "FSMTConfig: (<class 'transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer'>, None)\n",
      "FunnelConfig: (<class 'transformers.models.funnel.tokenization_funnel.FunnelTokenizer'>, <class 'transformers.models.funnel.tokenization_funnel_fast.FunnelTokenizerFast'>)\n",
      "GemmaConfig: (None, <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>)\n",
      "Gemma2Config: (None, <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>)\n",
      "Gemma3Config: (None, <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>)\n",
      "Gemma3TextConfig: (None, <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>)\n",
      "GitConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "GlmConfig: (None, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>)\n",
      "Glm4Config: (None, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>)\n",
      "GPT2Config: (None, None)\n",
      "GPT2Config: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "GPTBigCodeConfig: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "GPTNeoConfig: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "GPTNeoXConfig: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "GPTNeoXJapaneseConfig: (<class 'transformers.models.gpt_neox_japanese.tokenization_gpt_neox_japanese.GPTNeoXJapaneseTokenizer'>, None)\n",
      "GPTJConfig: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "GPTSanJapaneseConfig: (<class 'transformers.models.deprecated.gptsan_japanese.tokenization_gptsan_japanese.GPTSanJapaneseTokenizer'>, None)\n",
      "GroundingDinoConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "GroupViTConfig: (<class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>, <class 'transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast'>)\n",
      "HeliumConfig: (None, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>)\n",
      "HubertConfig: (<class 'transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer'>, None)\n",
      "IBertConfig: (<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, <class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>)\n",
      "IdeficsConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "Idefics2Config: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "Idefics3Config: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "InstructBlipConfig: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "InstructBlipVideoConfig: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "JambaConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "JetMoeConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "JukeboxConfig: (<class 'transformers.models.deprecated.jukebox.tokenization_jukebox.JukeboxTokenizer'>, None)\n",
      "Kosmos2Config: (None, <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>)\n",
      "LayoutLMConfig: (<class 'transformers.models.layoutlm.tokenization_layoutlm.LayoutLMTokenizer'>, <class 'transformers.models.layoutlm.tokenization_layoutlm_fast.LayoutLMTokenizerFast'>)\n",
      "LayoutLMv2Config: (<class 'transformers.models.layoutlmv2.tokenization_layoutlmv2.LayoutLMv2Tokenizer'>, <class 'transformers.models.layoutlmv2.tokenization_layoutlmv2_fast.LayoutLMv2TokenizerFast'>)\n",
      "LayoutLMv3Config: (<class 'transformers.models.layoutlmv3.tokenization_layoutlmv3.LayoutLMv3Tokenizer'>, <class 'transformers.models.layoutlmv3.tokenization_layoutlmv3_fast.LayoutLMv3TokenizerFast'>)\n",
      "LEDConfig: (<class 'transformers.models.led.tokenization_led.LEDTokenizer'>, <class 'transformers.models.led.tokenization_led_fast.LEDTokenizerFast'>)\n",
      "LiltConfig: (<class 'transformers.models.layoutlmv3.tokenization_layoutlmv3.LayoutLMv3Tokenizer'>, <class 'transformers.models.layoutlmv3.tokenization_layoutlmv3_fast.LayoutLMv3TokenizerFast'>)\n",
      "LlamaConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "Llama4Config: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "Llama4TextConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "LlavaConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "LlavaNextConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "LlavaNextVideoConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "LlavaOnevisionConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "LongformerConfig: (<class 'transformers.models.longformer.tokenization_longformer.LongformerTokenizer'>, <class 'transformers.models.longformer.tokenization_longformer_fast.LongformerTokenizerFast'>)\n",
      "LongT5Config: (None, <class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>)\n",
      "LukeConfig: (<class 'transformers.models.luke.tokenization_luke.LukeTokenizer'>, None)\n",
      "LxmertConfig: (<class 'transformers.models.lxmert.tokenization_lxmert.LxmertTokenizer'>, <class 'transformers.models.lxmert.tokenization_lxmert_fast.LxmertTokenizerFast'>)\n",
      "M2M100Config: (None, None)\n",
      "MambaConfig: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "Mamba2Config: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "MarianConfig: (None, None)\n",
      "MBartConfig: (None, <class 'transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast'>)\n",
      "MegaConfig: (<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, <class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>)\n",
      "MegatronBertConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "MgpstrConfig: (<class 'transformers.models.mgp_str.tokenization_mgp_str.MgpstrTokenizer'>, None)\n",
      "MistralConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "MixtralConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "MllamaConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "MobileBertConfig: (<class 'transformers.models.mobilebert.tokenization_mobilebert.MobileBertTokenizer'>, <class 'transformers.models.mobilebert.tokenization_mobilebert_fast.MobileBertTokenizerFast'>)\n",
      "ModernBertConfig: (None, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>)\n",
      "MoonshineConfig: (None, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>)\n",
      "MoshiConfig: (None, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>)\n",
      "MPNetConfig: (<class 'transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer'>, <class 'transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast'>)\n",
      "MptConfig: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "MraConfig: (<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, <class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>)\n",
      "MT5Config: (None, <class 'transformers.models.mt5.tokenization_mt5_fast.MT5TokenizerFast'>)\n",
      "MusicgenConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.T5Tokenizer'>, <class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>)\n",
      "MusicgenMelodyConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.T5Tokenizer'>, <class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>)\n",
      "MvpConfig: (<class 'transformers.models.mvp.tokenization_mvp.MvpTokenizer'>, <class 'transformers.models.mvp.tokenization_mvp_fast.MvpTokenizerFast'>)\n",
      "NemotronConfig: (None, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>)\n",
      "NezhaConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "NllbMoeConfig: (None, <class 'transformers.models.nllb.tokenization_nllb_fast.NllbTokenizerFast'>)\n",
      "NystromformerConfig: (None, <class 'transformers.models.albert.tokenization_albert_fast.AlbertTokenizerFast'>)\n",
      "OlmoConfig: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "Olmo2Config: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "OlmoeConfig: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "OmDetTurboConfig: (<class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>, <class 'transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast'>)\n",
      "OneFormerConfig: (<class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>, <class 'transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast'>)\n",
      "OpenAIGPTConfig: (<class 'transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer'>, <class 'transformers.models.openai.tokenization_openai_fast.OpenAIGPTTokenizerFast'>)\n",
      "OPTConfig: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "Owlv2Config: (<class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>, <class 'transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast'>)\n",
      "OwlViTConfig: (<class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>, <class 'transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast'>)\n",
      "PaliGemmaConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "PegasusConfig: (None, <class 'transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast'>)\n",
      "PegasusXConfig: (None, <class 'transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast'>)\n",
      "PerceiverConfig: (<class 'transformers.models.perceiver.tokenization_perceiver.PerceiverTokenizer'>, None)\n",
      "PersimmonConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "PhiConfig: (<class 'transformers.models.codegen.tokenization_codegen.CodeGenTokenizer'>, <class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>)\n",
      "Phi3Config: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "PhimoeConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "Pix2StructConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.T5Tokenizer'>, <class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>)\n",
      "PixtralVisionConfig: (None, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>)\n",
      "PLBartConfig: (None, None)\n",
      "ProphetNetConfig: (<class 'transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer'>, None)\n",
      "QDQBertConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "Qwen2Config: (<class 'transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer'>, <class 'transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast'>)\n",
      "Qwen2_5_VLConfig: (<class 'transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer'>, <class 'transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast'>)\n",
      "Qwen2AudioConfig: (<class 'transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer'>, <class 'transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast'>)\n",
      "Qwen2MoeConfig: (<class 'transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer'>, <class 'transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast'>)\n",
      "Qwen2VLConfig: (<class 'transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer'>, <class 'transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast'>)\n",
      "Qwen3Config: (<class 'transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer'>, <class 'transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast'>)\n",
      "Qwen3MoeConfig: (<class 'transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer'>, <class 'transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast'>)\n",
      "RagConfig: (<class 'transformers.models.rag.tokenization_rag.RagTokenizer'>, None)\n",
      "RealmConfig: (<class 'transformers.models.deprecated.realm.tokenization_realm.RealmTokenizer'>, <class 'transformers.models.deprecated.realm.tokenization_realm_fast.RealmTokenizerFast'>)\n",
      "RecurrentGemmaConfig: (None, <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>)\n",
      "ReformerConfig: (None, <class 'transformers.models.reformer.tokenization_reformer_fast.ReformerTokenizerFast'>)\n",
      "RemBertConfig: (None, <class 'transformers.models.rembert.tokenization_rembert_fast.RemBertTokenizerFast'>)\n",
      "RetriBertConfig: (<class 'transformers.models.deprecated.retribert.tokenization_retribert.RetriBertTokenizer'>, <class 'transformers.models.deprecated.retribert.tokenization_retribert_fast.RetriBertTokenizerFast'>)\n",
      "RobertaConfig: (<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, <class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>)\n",
      "RobertaPreLayerNormConfig: (<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, <class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>)\n",
      "RoCBertConfig: (<class 'transformers.models.roc_bert.tokenization_roc_bert.RoCBertTokenizer'>, None)\n",
      "RoFormerConfig: (<class 'transformers.models.roformer.tokenization_roformer.RoFormerTokenizer'>, <class 'transformers.models.roformer.tokenization_roformer_fast.RoFormerTokenizerFast'>)\n",
      "RwkvConfig: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "SeamlessM4TConfig: (None, <class 'transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast'>)\n",
      "SeamlessM4Tv2Config: (None, <class 'transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast'>)\n",
      "ShieldGemma2Config: (None, <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>)\n",
      "SiglipConfig: (None, None)\n",
      "Siglip2Config: (None, <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>)\n",
      "Speech2TextConfig: (None, None)\n",
      "Speech2Text2Config: (<class 'transformers.models.deprecated.speech_to_text_2.tokenization_speech_to_text_2.Speech2Text2Tokenizer'>, None)\n",
      "SpeechT5Config: (None, None)\n",
      "SplinterConfig: (<class 'transformers.models.splinter.tokenization_splinter.SplinterTokenizer'>, <class 'transformers.models.splinter.tokenization_splinter_fast.SplinterTokenizerFast'>)\n",
      "SqueezeBertConfig: (<class 'transformers.models.squeezebert.tokenization_squeezebert.SqueezeBertTokenizer'>, <class 'transformers.models.squeezebert.tokenization_squeezebert_fast.SqueezeBertTokenizerFast'>)\n",
      "StableLmConfig: (None, <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>)\n",
      "Starcoder2Config: (<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>)\n",
      "SwitchTransformersConfig: (None, <class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>)\n",
      "T5Config: (None, <class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>)\n",
      "TapasConfig: (<class 'transformers.models.tapas.tokenization_tapas.TapasTokenizer'>, None)\n",
      "TransfoXLConfig: (<class 'transformers.models.deprecated.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer'>, None)\n",
      "TvpConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "UdopConfig: (None, <class 'transformers.models.udop.tokenization_udop_fast.UdopTokenizerFast'>)\n",
      "UMT5Config: (None, <class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>)\n",
      "VideoLlavaConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "ViltConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "VipLlavaConfig: (<class 'transformers.utils.dummy_sentencepiece_objects.LlamaTokenizer'>, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "VisualBertConfig: (<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>)\n",
      "VitsConfig: (<class 'transformers.models.vits.tokenization_vits.VitsTokenizer'>, None)\n",
      "Wav2Vec2Config: (<class 'transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer'>, None)\n",
      "Wav2Vec2BertConfig: (<class 'transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer'>, None)\n",
      "Wav2Vec2ConformerConfig: (<class 'transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer'>, None)\n",
      "WhisperConfig: (<class 'transformers.models.whisper.tokenization_whisper.WhisperTokenizer'>, <class 'transformers.models.whisper.tokenization_whisper_fast.WhisperTokenizerFast'>)\n",
      "XCLIPConfig: (<class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>, <class 'transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast'>)\n",
      "XGLMConfig: (None, <class 'transformers.models.xglm.tokenization_xglm_fast.XGLMTokenizerFast'>)\n",
      "XLMConfig: (<class 'transformers.models.xlm.tokenization_xlm.XLMTokenizer'>, None)\n",
      "XLMProphetNetConfig: (None, None)\n",
      "XLMRobertaConfig: (None, <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>)\n",
      "XLMRobertaXLConfig: (None, <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>)\n",
      "XLNetConfig: (None, <class 'transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast'>)\n",
      "XmodConfig: (None, <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>)\n",
      "YosoConfig: (None, <class 'transformers.models.albert.tokenization_albert_fast.AlbertTokenizerFast'>)\n",
      "ZambaConfig: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n",
      "Zamba2Config: (None, <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers.models.auto.tokenization_auto import TOKENIZER_MAPPING\n",
    "\n",
    "# 查看所有注册的分词器映射\n",
    "for model_config, tokenizer_class in TOKENIZER_MAPPING.items():\n",
    "    print(f\"{model_config.__name__}: {tokenizer_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始生成 1000 个问题...\n",
      "难度: easy\n",
      "文件名: GSM_data_easy_1000.jsonl\n",
      "验证: 当前运行时模式 USE_MOD = False\n",
      "------------------------------\n",
      "已生成并保存 1000/1000 个问题...\n",
      "\n",
      "成功完成: GSM_data_easy_1000.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "def set_global_mod_mode(use_mod: bool):\n",
    "    \"\"\"\n",
    "    直接读取、修改并重写 const/params.py 文件来设置全局模式。\n",
    "    \"\"\"\n",
    "    params_path = os.path.join('const', 'params.py')\n",
    "    \n",
    "    try:\n",
    "        with open(params_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 无法找到 '{params_path}'。请确保脚本在 iGSM 项目根目录下运行。\")\n",
    "        exit(1)\n",
    "\n",
    "    found_line = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.match(r'^\\s*USE_MOD\\s*=', line):\n",
    "            lines[i] = f\"USE_MOD = {use_mod}\\n\"\n",
    "            found_line = True\n",
    "            break\n",
    "            \n",
    "    if not found_line:\n",
    "        lines.append(f\"\\nUSE_MOD = {use_mod}\\n\")\n",
    "\n",
    "    with open(params_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "\n",
    "def run_generation(num_problems, difficulty, output_file, seed=42, batch_size=1000):\n",
    "    \"\"\"\n",
    "    封装了导入和生成逻辑的主函数。\n",
    "    \"\"\"\n",
    "    from data_gen.pretrain.id_gen import IdGen\n",
    "    from tools.tools import tokenizer, fix_seed\n",
    "    from const import params\n",
    "\n",
    "    # fix_seed(seed)\n",
    "    data_buffer, total_generated = [], 0\n",
    "\n",
    "    print(f\"开始生成 {num_problems} 个问题...\")\n",
    "    print(f\"难度: {difficulty}\")\n",
    "    print(f\"文件名: {output_file}\")\n",
    "    print(f\"验证: 当前运行时模式 USE_MOD = {params.USE_MOD}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # vvvvvvvv 关键修正点 vvvvvvvv\n",
    "        # vvvvvvvv 关键修正点 vvvvvvvv\n",
    "    def get_prob_sol_ans_triple(tpy: Literal[\"easy\", \"med\", \"hard\"]):\n",
    "        assert tpy in [\"easy\", \"med\", \"hard\"], \"Invalid type: Choose 'easy', 'med' or 'hard'\"\n",
    "        \n",
    "        if tpy == \"easy\":\n",
    "            max_op, max_edge = (9, 12)\n",
    "        elif tpy == \"med\":\n",
    "            max_op, max_edge = (15, 20)\n",
    "        else:  # hard\n",
    "            max_op, max_edge = (21, 28)\n",
    "\n",
    "        id_gen = IdGen(max_op=max_op, max_edge=max_edge, perm_level=5, detail_level=0)\n",
    "        \n",
    "        # 根据当前模式决定传递给 gen_prob 的哈希列表\n",
    "        if params.USE_MOD:\n",
    "            valid_hashes = list(range(23))  # 取模模式\n",
    "        else:\n",
    "            valid_hashes = [0]  # 非取模模式\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                id_gen.gen_prob(valid_hashes, p_format=\"pq\")\n",
    "                break \n",
    "            except Exception:\n",
    "                continue\n",
    "        return id_gen\n",
    "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for i in range(num_problems):\n",
    "            id_gen = get_prob_sol_ans_triple(difficulty)\n",
    "            problem_text = tokenizer.decode(id_gen.prob_token, skip_special_tokens=True).strip()\n",
    "            solution_text = tokenizer.decode(id_gen.sol_token, skip_special_tokens=True).strip()\n",
    "            answer_text = tokenizer.decode(id_gen.ans_token, skip_special_tokens=True).strip()\n",
    "            \n",
    "            data_buffer.append({\n",
    "                \"id\": f\"{difficulty}_{seed}_{i}\",\n",
    "                \"problem\": problem_text, \"solution\": solution_text, \"answer\": answer_text\n",
    "            })\n",
    "            \n",
    "            if len(data_buffer) >= batch_size or (i + 1) == num_problems:\n",
    "                for item in data_buffer:\n",
    "                    f_out.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                total_generated += len(data_buffer)\n",
    "                print(f\"已生成并保存 {total_generated}/{num_problems} 个问题...\")\n",
    "                data_buffer = []\n",
    "            \n",
    "    print(f\"\\n成功完成: {output_file}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "#                      主程序入口\n",
    "# ===================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 所有可调参数都集中在这里 ---\n",
    "    USE_MOD = False\n",
    "    NUM_TO_GENERATE = 1000\n",
    "    DIFFICULTY = \"easy\"\n",
    "    OUTPUT_FILE = \"GSM_data_easy_1000.jsonl\"\n",
    "    BATCH_SIZE = 1000\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # --- 执行 ---\n",
    "    set_global_mod_mode(USE_MOD)\n",
    "    run_generation(\n",
    "        num_problems=NUM_TO_GENERATE,\n",
    "        difficulty=DIFFICULTY,\n",
    "        output_file=OUTPUT_FILE,\n",
    "        seed=RANDOM_SEED,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 😺🤖直出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始生成 1000000 个问题...\n",
      "难度: easy\n",
      "文件名: GSM_data_easy_10000002.jsonl\n",
      "验证: 当前运行时模式 USE_MOD = False\n",
      "------------------------------\n",
      "已生成并保存 10000/1000000 个问题...\n",
      "已生成并保存 20000/1000000 个问题...\n",
      "已生成并保存 30000/1000000 个问题...\n",
      "已生成并保存 40000/1000000 个问题...\n",
      "已生成并保存 50000/1000000 个问题...\n",
      "已生成并保存 60000/1000000 个问题...\n",
      "已生成并保存 70000/1000000 个问题...\n",
      "已生成并保存 80000/1000000 个问题...\n",
      "已生成并保存 90000/1000000 个问题...\n",
      "已生成并保存 100000/1000000 个问题...\n",
      "已生成并保存 110000/1000000 个问题...\n",
      "已生成并保存 120000/1000000 个问题...\n",
      "已生成并保存 130000/1000000 个问题...\n",
      "已生成并保存 140000/1000000 个问题...\n",
      "已生成并保存 150000/1000000 个问题...\n",
      "已生成并保存 160000/1000000 个问题...\n",
      "已生成并保存 170000/1000000 个问题...\n",
      "已生成并保存 180000/1000000 个问题...\n",
      "已生成并保存 190000/1000000 个问题...\n",
      "已生成并保存 200000/1000000 个问题...\n",
      "已生成并保存 210000/1000000 个问题...\n",
      "已生成并保存 220000/1000000 个问题...\n",
      "已生成并保存 230000/1000000 个问题...\n",
      "已生成并保存 240000/1000000 个问题...\n",
      "已生成并保存 250000/1000000 个问题...\n",
      "已生成并保存 260000/1000000 个问题...\n",
      "已生成并保存 270000/1000000 个问题...\n",
      "已生成并保存 280000/1000000 个问题...\n",
      "已生成并保存 290000/1000000 个问题...\n",
      "已生成并保存 300000/1000000 个问题...\n",
      "已生成并保存 310000/1000000 个问题...\n",
      "已生成并保存 320000/1000000 个问题...\n",
      "已生成并保存 330000/1000000 个问题...\n",
      "已生成并保存 340000/1000000 个问题...\n",
      "已生成并保存 350000/1000000 个问题...\n",
      "已生成并保存 360000/1000000 个问题...\n",
      "已生成并保存 370000/1000000 个问题...\n",
      "已生成并保存 380000/1000000 个问题...\n",
      "已生成并保存 390000/1000000 个问题...\n",
      "已生成并保存 400000/1000000 个问题...\n",
      "已生成并保存 410000/1000000 个问题...\n",
      "已生成并保存 420000/1000000 个问题...\n",
      "已生成并保存 430000/1000000 个问题...\n",
      "已生成并保存 440000/1000000 个问题...\n",
      "已生成并保存 450000/1000000 个问题...\n",
      "已生成并保存 460000/1000000 个问题...\n",
      "已生成并保存 470000/1000000 个问题...\n",
      "已生成并保存 480000/1000000 个问题...\n",
      "已生成并保存 490000/1000000 个问题...\n",
      "已生成并保存 500000/1000000 个问题...\n",
      "已生成并保存 510000/1000000 个问题...\n",
      "已生成并保存 520000/1000000 个问题...\n",
      "已生成并保存 530000/1000000 个问题...\n",
      "已生成并保存 540000/1000000 个问题...\n",
      "已生成并保存 550000/1000000 个问题...\n",
      "已生成并保存 560000/1000000 个问题...\n",
      "已生成并保存 570000/1000000 个问题...\n",
      "已生成并保存 580000/1000000 个问题...\n",
      "已生成并保存 590000/1000000 个问题...\n",
      "已生成并保存 600000/1000000 个问题...\n",
      "已生成并保存 610000/1000000 个问题...\n",
      "已生成并保存 620000/1000000 个问题...\n",
      "已生成并保存 630000/1000000 个问题...\n",
      "已生成并保存 640000/1000000 个问题...\n",
      "已生成并保存 650000/1000000 个问题...\n",
      "已生成并保存 660000/1000000 个问题...\n",
      "已生成并保存 670000/1000000 个问题...\n",
      "已生成并保存 680000/1000000 个问题...\n",
      "已生成并保存 690000/1000000 个问题...\n",
      "已生成并保存 700000/1000000 个问题...\n",
      "已生成并保存 710000/1000000 个问题...\n",
      "已生成并保存 720000/1000000 个问题...\n",
      "已生成并保存 730000/1000000 个问题...\n",
      "已生成并保存 740000/1000000 个问题...\n",
      "已生成并保存 750000/1000000 个问题...\n",
      "已生成并保存 760000/1000000 个问题...\n",
      "已生成并保存 770000/1000000 个问题...\n",
      "已生成并保存 780000/1000000 个问题...\n",
      "已生成并保存 790000/1000000 个问题...\n",
      "已生成并保存 800000/1000000 个问题...\n",
      "已生成并保存 810000/1000000 个问题...\n",
      "已生成并保存 820000/1000000 个问题...\n",
      "已生成并保存 830000/1000000 个问题...\n",
      "已生成并保存 840000/1000000 个问题...\n",
      "已生成并保存 850000/1000000 个问题...\n",
      "已生成并保存 860000/1000000 个问题...\n",
      "已生成并保存 870000/1000000 个问题...\n",
      "已生成并保存 880000/1000000 个问题...\n",
      "已生成并保存 890000/1000000 个问题...\n",
      "已生成并保存 900000/1000000 个问题...\n",
      "已生成并保存 910000/1000000 个问题...\n",
      "已生成并保存 920000/1000000 个问题...\n",
      "已生成并保存 930000/1000000 个问题...\n",
      "已生成并保存 940000/1000000 个问题...\n",
      "已生成并保存 950000/1000000 个问题...\n",
      "已生成并保存 960000/1000000 个问题...\n",
      "已生成并保存 970000/1000000 个问题...\n",
      "已生成并保存 980000/1000000 个问题...\n",
      "已生成并保存 990000/1000000 个问题...\n",
      "已生成并保存 1000000/1000000 个问题...\n",
      "\n",
      "成功完成: GSM_data_easy_10000002.jsonl\n",
      "[*] 开始转换文件...\n",
      "    输入: GSM_data_easy_10000002.jsonl\n",
      "    输出: GSM_data_easy_1000000_😺🤖2.jsonl\n",
      "转换中: |          | 1000000/? [00:15<00:00]\n",
      "\n",
      "==============================\n",
      " 转换完成 \n",
      "==============================\n",
      "[*] 共处理输入文件行数: 1,000,000\n",
      "[*] 成功转换并写入行数: 1,000,000\n",
      "[*] 跳过或转换失败行数: 0\n",
      "[*] 结果已保存至: GSM_data_easy_1000000_😺🤖2.jsonl\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from typing import Literal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==============================================================\n",
    "#                   数据生成部分\n",
    "# ==============================================================\n",
    "\n",
    "def set_global_mod_mode(use_mod: bool):\n",
    "    \"\"\"\n",
    "    直接读取、修改并重写 const/params.py 文件来设置全局模式。\n",
    "    \"\"\"\n",
    "    params_path = os.path.join('const', 'params.py')\n",
    "    \n",
    "    try:\n",
    "        with open(params_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 无法找到 '{params_path}'。请确保脚本在 iGSM 项目根目录下运行。\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    found_line = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.match(r'^\\s*USE_MOD\\s*=', line):\n",
    "            lines[i] = f\"USE_MOD = {use_mod}\\n\"\n",
    "            found_line = True\n",
    "            break\n",
    "            \n",
    "    if not found_line:\n",
    "        lines.append(f\"\\nUSE_MOD = {use_mod}\\n\")\n",
    "\n",
    "    with open(params_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "\n",
    "def run_generation(num_problems, difficulty, output_file, seed=42, batch_size=1000):\n",
    "    \"\"\"\n",
    "    封装了导入和生成逻辑的主函数。\n",
    "    \"\"\"\n",
    "    from data_gen.pretrain.id_gen import IdGen\n",
    "    from tools.tools import tokenizer\n",
    "    from const import params\n",
    "\n",
    "    data_buffer, total_generated = [], 0\n",
    "\n",
    "    print(f\"开始生成 {num_problems} 个问题...\")\n",
    "    print(f\"难度: {difficulty}\")\n",
    "    print(f\"文件名: {output_file}\")\n",
    "    print(f\"验证: 当前运行时模式 USE_MOD = {params.USE_MOD}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # vvvvvvvv 支持 easy/med/hard vvvvvvvv\n",
    "    def get_prob_sol_ans_triple(tpy: Literal[\"easy\", \"med\", \"hard\"]):\n",
    "        assert tpy in [\"veasy\",\"easy\", \"med\", \"hard\"], \"Invalid type: Choose  'veasy','easy', 'med' or 'hard'\"\n",
    "        \n",
    "        if tpy == \"veasy\":\n",
    "            max_op, max_edge = (5, 8)\n",
    "        elif tpy == \"easy\":\n",
    "            max_op, max_edge = (9, 12)\n",
    "        elif tpy == \"med\":\n",
    "            max_op, max_edge = (15, 20)\n",
    "        else:  # hard\n",
    "            max_op, max_edge = (21, 28)\n",
    "\n",
    "        id_gen = IdGen(max_op=max_op, max_edge=max_edge, perm_level=5, detail_level=0)\n",
    "        \n",
    "        # 根据当前模式决定传递给 gen_prob 的哈希列表\n",
    "        valid_hashes = list(range(23)) if params.USE_MOD else [0]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                id_gen.gen_prob(valid_hashes, p_format=\"pq\")\n",
    "                break \n",
    "            except Exception:\n",
    "                continue\n",
    "        return id_gen\n",
    "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for i in range(num_problems):\n",
    "            id_gen = get_prob_sol_ans_triple(difficulty)\n",
    "            problem_text = tokenizer.decode(id_gen.prob_token, skip_special_tokens=True).strip()\n",
    "            solution_text = tokenizer.decode(id_gen.sol_token, skip_special_tokens=True).strip()\n",
    "            answer_text = tokenizer.decode(id_gen.ans_token, skip_special_tokens=True).strip()\n",
    "            \n",
    "            data_buffer.append({\n",
    "                \"id\": f\"{difficulty}_{seed}_{i}\",\n",
    "                \"problem\": problem_text, \"solution\": solution_text, \"answer\": answer_text\n",
    "            })\n",
    "            \n",
    "            if len(data_buffer) >= batch_size or (i + 1) == num_problems:\n",
    "                for item in data_buffer:\n",
    "                    f_out.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                total_generated += len(data_buffer)\n",
    "                print(f\"已生成并保存 {total_generated}/{num_problems} 个问题...\")\n",
    "                data_buffer = []\n",
    "            \n",
    "    print(f\"\\n成功完成: {output_file}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "#                   转换为 😺🤖 对话格式\n",
    "# ==============================================================\n",
    "\n",
    "def convert_to_conversation(data: dict) -> list | None:\n",
    "    \"\"\"\n",
    "    将单行原始JSON对象转换为目标对话格式 [{\"😺\": ...}, {\"🤖\": ...}]。\n",
    "    \"\"\"\n",
    "    problem_text = data.get(\"problem\")\n",
    "    solution_text = data.get(\"solution\")\n",
    "    answer_text = data.get(\"answer\")\n",
    "\n",
    "    if not all([problem_text, solution_text, answer_text]):\n",
    "        return None\n",
    "\n",
    "    assistant_content = solution_text.strip() + \"\\n\" + answer_text.strip()\n",
    "    conversation = [\n",
    "        {\"😺\": problem_text.strip()},\n",
    "        {\"🤖\": assistant_content}\n",
    "    ]\n",
    "    return conversation\n",
    "\n",
    "\n",
    "def convert_file(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    将生成的数据文件转换为对话格式。\n",
    "    \"\"\"\n",
    "    print(f\"[*] 开始转换文件...\")\n",
    "    print(f\"    输入: {input_file}\")\n",
    "    print(f\"    输出: {output_file}\")\n",
    "\n",
    "    lines_processed = 0\n",
    "    lines_converted = 0\n",
    "    lines_skipped = 0\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
    "             open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "            \n",
    "            pbar = tqdm(f_in, desc=\"转换中\", unit=\"行\", file=sys.stdout, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "\n",
    "            for line in pbar:\n",
    "                lines_processed += 1\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    lines_skipped += 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    original_data = json.loads(line)\n",
    "                    converted_conversation = convert_to_conversation(original_data)\n",
    "\n",
    "                    if converted_conversation:\n",
    "                        output_line = json.dumps(\n",
    "                            converted_conversation, \n",
    "                            ensure_ascii=False, \n",
    "                            separators=(',', ':')\n",
    "                        )\n",
    "                        f_out.write(output_line + '\\n')\n",
    "                        lines_converted += 1\n",
    "                    else:\n",
    "                        lines_skipped += 1\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"\\n[!] 警告: 第 {lines_processed} 行 JSON 解析失败，已跳过。\", file=sys.stderr)\n",
    "                    lines_skipped += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n[!] 错误: 处理第 {lines_processed} 行时发生未知错误: {e}\", file=sys.stderr)\n",
    "                    lines_skipped += 1\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n[X] 错误: 输入文件 '{input_file}' 未找到。\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\" 转换完成 \")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"[*] 共处理输入文件行数: {lines_processed:,}\")\n",
    "    print(f\"[*] 成功转换并写入行数: {lines_converted:,}\")\n",
    "    print(f\"[*] 跳过或转换失败行数: {lines_skipped:,}\")\n",
    "    print(f\"[*] 结果已保存至: {output_file}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "#                      主程序入口\n",
    "# ==============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 生成参数 ---\n",
    "    USE_MOD = False\n",
    "    NUM_TO_GENERATE = 1000000\n",
    "    DIFFICULTY = \"easy\"   # 可选: \"veasy\" | \"easy\" | \"med\" | \"hard\"\n",
    "    GEN_OUTPUT_FILE = f\"GSM_data_{DIFFICULTY}_{NUM_TO_GENERATE}2.jsonl\"\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # --- 转换参数 ---\n",
    "    FINAL_OUTPUT_FILE = f\"GSM_data_{DIFFICULTY}_{NUM_TO_GENERATE}_😺🤖2.jsonl\"\n",
    "\n",
    "    # --- 执行数据生成 ---\n",
    "    set_global_mod_mode(USE_MOD)\n",
    "    run_generation(\n",
    "        num_problems=NUM_TO_GENERATE,\n",
    "        difficulty=DIFFICULTY,\n",
    "        output_file=GEN_OUTPUT_FILE,\n",
    "        seed=RANDOM_SEED,\n",
    "        batch_size=10000\n",
    "    )\n",
    "\n",
    "    # --- 执行文件转换 ---\n",
    "    convert_file(GEN_OUTPUT_FILE, FINAL_OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM_data_veasy__1000_😺🤖.jsonl : 650.41\n",
      "GSM_data_easy__1000_😺🤖.jsonl : 902.75\n",
      "GSM_data_med_500000_😺🤖.jsonl : 1258.97\n",
      "GSM_data_hard_500000_😺🤖.jsonl : 1556.80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def avg_line_length(file_path, max_lines=1000):\n",
    "    total_len = 0\n",
    "    count = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_lines:\n",
    "                break\n",
    "            total_len += len(line.strip())\n",
    "            count += 1\n",
    "    return total_len / count if count > 0 else 0\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "    for fname in os.listdir(\".\"):\n",
    "        if \"😺🤖\" in fname and fname.endswith(\".jsonl\"):\n",
    "            avg_len = avg_line_length(fname, max_lines=1000)\n",
    "            results.append((fname, avg_len))\n",
    "    \n",
    "    # 排序（从短到长）\n",
    "    results.sort(key=lambda x: x[1])\n",
    "    \n",
    "    for fname, avg_len in results:\n",
    "        print(f\"{fname} : {avg_len:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
